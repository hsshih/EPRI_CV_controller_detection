{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EPRI_ObjectDetection_Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVdTgdXPVNl1"
      },
      "source": [
        "# Object Detection Training Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADwHIXuRVUJW"
      },
      "source": [
        "## Introductory Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhFKkYFhXPfv"
      },
      "source": [
        "This notebook serves as a template on how to train an EfficientDet model for detecting cotrollers using the TensorFlow Object Detection API. The models use our synthetic data for training. The trained models are then exported and can be used for evaluation.\n",
        "\n",
        "For the code in thise notebook to work the TF Object Detection API and a few standard modules need to be installed - see the README-file in the project main directory for more detailed information.\n",
        "\n",
        "The following files, included in what is provided by the EPRI-CV team, are needed:\n",
        "*   __src/preproc.py__ - this module contains the functions that are used to preprocess the data\n",
        "*   __src/visualize.py__ - this module contains functions to visualize different aspects of the evaluation\n",
        "*   __src/utils.py__ - utils for writing/reading XML annotations\n",
        "*   __src/generate_tfrecord.py__ - script to generate tfrecord files from image data and XML annotations\n",
        "\n",
        "Assumed project structure:\n",
        "```\n",
        "main_project_dir/\n",
        "├─ README.md                               <- The top-level README you are currently reading.\n",
        "├─ generate-synthetic-images/              <- the scripts to generate synthetic images that will serve as training\n",
        "├─ data/                  \n",
        "│   ├─ test-annotated-images/              <- test images and the xml files with their labels\n",
        "│   ├─ synthetic-annotated-train-images/   <- synthetic images used for training and the xml files with their labels\n",
        "│   ├─ train-valid-split\n",
        "│   │    ├─ training/                      <- training images and the xml with their labels used for training\n",
        "│   │    ├─ validation/                    <- validation images and the xml with their labels used for training\n",
        "│   │    ├─ train.tfrec                    <- tfrecord of the training images and labels\n",
        "│   │    ├─ valid.tfrec                    <- tfrecord of the validation images and labels\n",
        "│   │    └─ label_map.pbtxt                <- label map for this training/validation split\n",
        "├─ models/                              \n",
        "│   ├─ pre-trained/                        <- pre-trained models downlowaded from the Tensorflow API zoo\n",
        "│   ├─ fine-tuned/                         <- fine-tuned models, obtained from training with train-valid-split\n",
        "│   └─ default-pipeline-configs/           <- pipeline configuration files for our models\n",
        "├─ notebooks/                              <- notebooks used for training and evaluation\n",
        "│   ├─ EPRI_ObjectDetection_Training.ipynb\n",
        "│   └─ EPRI_ObjectDetection_Evaluation.ipynb\n",
        "└─ src/                                    <- scripts needed in the notebooks\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ktK0kommiR"
      },
      "source": [
        "# Preparation / initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmtN-Rzuu9j1"
      },
      "source": [
        "#### Set the root folder (assuming running from the `notebooks` folder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc-y0hTeswTq"
      },
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "ROOT_DIR = Path(\"/content/gdrive/MyDrive/epri-deliver\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuXqCO65vJuT"
      },
      "source": [
        "#### Set the path to the TF Object Detection API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krA6aMplvIKF"
      },
      "source": [
        "TF_DIR = ROOT_DIR/\"tf-models\" "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfOphtlYve26"
      },
      "source": [
        "#### Colab Init (REMOVE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmKdIOUspyA0",
        "outputId": "96d83ac6-b078-4f09-99a0-fdd601cf64a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/epri-deliver/models"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/MyDrive/epri-deliver/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0emcf7R9qS0d"
      },
      "source": [
        "if not TF_DIR.exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models $TF_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vzN5thItAeh",
        "outputId": "24fe98b3-382c-4a90-c54a-2b085b6d3c6d"
      },
      "source": [
        "%cd {TF_DIR/'research'}\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python setup.py build\n",
        "!python setup.py install\n",
        "%cd /content/gdrive/MyDrive/epri-deliver/data"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/epri-deliver/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOmDDQMTqT6M"
      },
      "source": [
        "import sys\n",
        "sys.path.append(str(TF_DIR/\"research\"))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKi4F-mNqXPl"
      },
      "source": [
        "!pip uninstall imgaug\n",
        "!pip install git+https://github.com/aleju/imgaug.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvS_V_-4mvh0"
      },
      "source": [
        "### Project folder structure: set paths to existing directories in the root folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_auWiThmzdE"
      },
      "source": [
        "DATA_DIR = ROOT_DIR/'data'\n",
        "# Folder containing unprocessed/unaugmented annotated synthesised data \n",
        "# (images + corresponding XML annotation files) used for training\n",
        "RAW_SYN_DATA_DIR = DATA_DIR/'synthetic-annotated-train-images'\n",
        "# Unprocessed annotated test data\n",
        "RAW_TEST_DATA_DIR = DATA_DIR/'test-annotated-images'\n",
        "\n",
        "# Processed (e.g. resized, augmented) training/validation data \n",
        "# ready for the model to use \n",
        "TRAIN_VALID_DIR = DATA_DIR/'train-valid-split'\n",
        "\n",
        "# Models location\n",
        "MODELS_DIR = ROOT_DIR/'models'\n",
        "# Pretrained models from TF model zoo\n",
        "PRE_MODELS_DIR = MODELS_DIR/'pre-trained'\n",
        "# Our fine-tuned models\n",
        "FT_MODELS_DIR = MODELS_DIR/'fine-tuned'\n",
        "DEFAULT_CONFIGS_DIR = MODELS_DIR/'default-pipeline-configs'\n",
        "\n",
        "# Add our source directory to python PATH\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu1IFOHHy95z"
      },
      "source": [
        "sys.path.append(str(ROOT_DIR/'src'))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nszx9Ad6lbew"
      },
      "source": [
        "### Choose a synthesised data folder used for training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB0xuFb_5o99"
      },
      "source": [
        "* Each image in the main synthesised dataset we use below contains 5 to 9 Siemens S7-300 and Allen Bradley controllers and 3 to 6 distractor Siemens ET200M controllers pasted onto backgrounds. \n",
        "* The scale of the controllers is adjusted to each background. We also introduce additional 10% random variations of size and 5-degree random rotations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsWwOSArt9j5"
      },
      "source": [
        "RAW_TRAIN_DATA_DIR = RAW_SYN_DATA_DIR/'bg_adjusted_with_distractors'"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsxEN7QxeoBJ"
      },
      "source": [
        "### Choose our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPXdUR1V8DT_"
      },
      "source": [
        "We find that three models with different levels of augmentation (explained further) produced best results in our experiments:\n",
        "* `efficient_det_1024_rand_aug_1_4`: 1024x1024 resolution with strong augmentation\n",
        "* `efficient_det_1024_rand_aug_1_2`: 1024x1024 resolution with medium augmentation\n",
        "* `efficient_det_1024_rand_aug_1_0`: 1024x1024 resolution with light augmentation\n",
        "\n",
        "The model with the strongest augmentation provides significantly better mAP score at 0.75 IOU (~50%, as opposed to ~30% for the model with light augmentation), meaning that it localizes the objects better. All the models have the same mAP=77% at 0.5 IOU, which is the most important metric if we care more about identification then precise localisation. The model with light augmentation however sometimes produces 1-2 less Allen Bradley non-detections. The model choice should be based on the available validation/testing data. Here we train the model with strong augmentation. \n",
        "\n",
        "All the models should use the same synthesised dataset in `data/synthetic-annotated-train-images/bg_adjusted_with_distractors` for best results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EG2pjpDogEA"
      },
      "source": [
        "# List of TF pre-trained models with download links\n",
        "PRE_MODELS = {\n",
        "    'efficient_det_1024': \"http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d4_coco17_tpu-32.tar.gz\",\n",
        "}\n",
        "# Select a pre-trained model\n",
        "PRE_MODEL_NAME = 'efficient_det_1024'\n",
        "# Set the name of our fine-tuned model\n",
        "MY_MODEL_NAME = 'efficient_det_1024_rand_aug_1_4'"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfz7YU5estud"
      },
      "source": [
        "### Global parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g33VS0sBszUq"
      },
      "source": [
        "# Target size of training/validation images after preprocessing\n",
        "IMAGE_SIZE = 1024\n",
        "# batch size (reduce if out of GPU memory)\n",
        "BATCH_SIZE = 32\n",
        "# number of steps for training\n",
        "NUM_STEPS = 6000"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH2ISgy1qYnN"
      },
      "source": [
        "### Initialize folder structure for custom training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2vukCeTt_Am"
      },
      "source": [
        "Make directories for customized processed training/validation data ready to be fed to the model. \n",
        "\n",
        "Set paths to tfrec files and the label map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2kc6VoRxwgi",
        "outputId": "37e8a5d6-0788-482d-a7f1-d78abf37c8fa"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/epri-deliver/data"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/epri-deliver/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6GR9O4RlYtn"
      },
      "source": [
        "# Directories for preprocessed annotated image data used for training/validation\n",
        "if not (TRAIN_VALID_DIR/'training').exists():\n",
        "  (TRAIN_VALID_DIR/'training').mkdir()\n",
        "if not (TRAIN_VALID_DIR/'validation').exists():\n",
        "  (TRAIN_VALID_DIR/'validation').mkdir()\n",
        "\n",
        "# Paths to TF record files containing training and validation data\n",
        "train_tfrec_path = TRAIN_VALID_DIR/'train.record'\n",
        "valid_tfrec_path = TRAIN_VALID_DIR/'valid.record'\n",
        "\n",
        "# Path to the label map file that contains class names and IDs\n",
        "label_map_path = TRAIN_VALID_DIR/'label_map.pbtxt'"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TvBS71Ve8Ub"
      },
      "source": [
        "Make directories and set paths for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj6iNjZ-e-2b"
      },
      "source": [
        "my_model_dir = FT_MODELS_DIR / MY_MODEL_NAME\n",
        "if not my_model_dir.exists(): my_model_dir.mkdir()\n",
        "\n",
        "# Path to the model configuration file\n",
        "config_path = my_model_dir / 'pipeline.config'\n",
        "\n",
        "# Path to the initial fine tune checkpoint (from the pre-trained model)\n",
        "ft_ckpt_dir = my_model_dir / 'fine_tune_checkpoint'\n",
        "if not ft_ckpt_dir.exists(): ft_ckpt_dir.mkdir()\n",
        "ft_ckpt_path = ft_ckpt_dir / 'ckpt-0'\n",
        "\n",
        "# Make a folder for exported model\n",
        "my_export_dir = my_model_dir/'exported'\n",
        "if not my_export_dir.exists():\n",
        "  my_export_dir.mkdir()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpDNd8ek6rBu"
      },
      "source": [
        "Set directory names specific for the current computation environment (TPU cluster or local GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lci2tObkMaA"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv5I4pMvzMUp",
        "outputId": "97c8602a-6736-4562-8b90-f85ffbf41261"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/epri-deliver"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/epri-deliver\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NjqGBqpkN2F"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math, os, shutil, glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import cv2\n",
        "\n",
        "# TF object detection API utils\n",
        "from object_detection.utils import label_map_util \n",
        "from object_detection.utils import config_util \n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "# Our src/ functions\n",
        "import src.utils as src_util\n",
        "import src.preproc as src_pre\n",
        "import src.visualize as src_viz\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0LaPEN-kXuk"
      },
      "source": [
        "from importlib import reload  \n",
        "src_viz = reload(src_viz)\n",
        "src_pre = reload(src_pre)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbgWpt4xcyiB"
      },
      "source": [
        "# Create and save the label map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikeaEpaDtQ5o"
      },
      "source": [
        "The label map file maps class IDs to class names. It is needed for the model to initialize. We save it in `TRAIN_VALID_DIR`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZN_LlGNc1yJ",
        "outputId": "bedc0016-871f-4458-f6d7-73b44c9955c8"
      },
      "source": [
        "%%writefile $label_map_path\n",
        "  item {\n",
        "    id: 1\n",
        "    name: 'AllenBradley'\n",
        "  }\n",
        "  item {\n",
        "    id: 2\n",
        "    name: 'Siemens'\n",
        "  }"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/gdrive/MyDrive/epri-deliver/data/train-valid-split/label_map.pbtxt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvQhKdfPPZrE"
      },
      "source": [
        "category_index = label_map_util.create_category_index_from_labelmap(label_map_path)\n",
        "label_map_dict = label_map_util.get_label_map_dict(str(label_map_path))\n",
        "# Number of classes extracted from the label map\n",
        "num_classes = len(label_map_dict.items())"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK3bH34Puam1"
      },
      "source": [
        "# Data preparation and augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJlzapAn-ty0"
      },
      "source": [
        "# clean up image data in our train/valid data folder if it's not empty \n",
        "src_pre.clear_dir(TRAIN_VALID_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ulSZa0n2EGM"
      },
      "source": [
        "## Training data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwu9moRS2Xq9"
      },
      "source": [
        "* Augmenting the training images by applying random transformations helps diversify the training set and avoid overfitting. \n",
        "* Without augmentation, the model quickly learns how to find controllers in our artificial images and is prone to relying on the unnatural artifacts in the synthesised data. This reduces performance on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOsTZs6N9aie"
      },
      "source": [
        "The augmentation strategy we use is RandAugment. For each synthesised image, we apply 2 types of augmentations randomly chosen from the list of ten transformations:\n",
        "* Rotation\n",
        "* Shear along X\n",
        "* Shear along Y\n",
        "* Brightness\n",
        "* Hue\n",
        "* Contrast\n",
        "* Saturation\n",
        "* Gaussian noise\n",
        "* Motion blur\n",
        "* Sharpness\n",
        "\n",
        "RandAugment has only two parameters:\n",
        "* Number of augmentation to choose (`rand_aug_num`)\n",
        "* Magnitude of augmentations (`rand_aug_mag` same for all)\n",
        "\n",
        "\n",
        "* Before augmentation, resize synthesised images to IMAGE_SIZE if needed.\n",
        "* We repeat the augmentation process `augment_mult` times to generate `augment_mult` times more images.\n",
        "* Annotation files are transformed together with the images.\n",
        "* Place the augmented data with annotations in `my_train_valid_dir/training`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgdgxO_UEct7",
        "outputId": "a4636b5a-82e6-45e8-851f-7a5b8e4427f5"
      },
      "source": [
        "# synthetic images/labels (no preprocessing required, proper size already)\n",
        "src_pre.copy_augment_data(\n",
        "    RAW_TRAIN_DATA_DIR, my_train_valid_dir/'training', \n",
        "    target_size = IMAGE_SIZE,\n",
        "    reshape2square = True,\n",
        "    augment_kwargs = {\n",
        "        'rand_augment' : True,\n",
        "        'rand_aug_mag' : 1.4,\n",
        "        'rand_aug_num' : 2\n",
        "    },\n",
        "    augment_mult = 10\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Produced 3000 images with labels of class all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgxd3azhFzw7",
        "outputId": "0ae4f9be-628b-4257-f7b9-de2a8f86f9ce"
      },
      "source": [
        "# test images/labels\n",
        "if os.path.exists('tmp'): shutil.rmtree('tmp')\n",
        "src_pre.make_split_images_and_labels(SRC_TST_DIR, 'tmp', only_top_left=True)\n",
        "src_mkdat.copy_augment_data('tmp', image_data_dir, \n",
        "                            class_subdirs=False, \n",
        "                            reshape2square='stretch',\n",
        "                            target_size = IMAGE_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Produced 45 images with labels of class all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE733EypbABg"
      },
      "source": [
        "### Make a training / validation split in `image_data` from the files currently in this folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rODxGJDL_L-w"
      },
      "source": [
        "# clear train/validation directories if they exist\n",
        "src_mkdat.clear_dir(image_data_dir/'training')\n",
        "src_mkdat.clear_dir(image_data_dir/'validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5WLxxHvJJyt"
      },
      "source": [
        "Splitting can be done in several steps to have different train/validate ratios for each data source: \n",
        "* Copying files from a single source to `image_data`\n",
        "* Distributing them between `image_data/training` and `image_data/validation` with a specific `train_valid_split` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raNaOc77asBy",
        "outputId": "77833442-7939-4d59-f12d-5d5b6f69ba6c"
      },
      "source": [
        "# make the split\n",
        "train_img_dir, valid_img_dir = src_mkdat.make_split(image_data_dir, train_valid_split=0.)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training examples: 0\n",
            "number of validation examples: 45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQLK9TUiblkd"
      },
      "source": [
        "### Convert images and .xml labels to .tfrec files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqVMfPdNbpig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487c416c-2497-4aa1-f19b-dc0e443a0c03"
      },
      "source": [
        "!python src/generate_tfrecord_sk.py -x $train_img_dir -l $label_map_path -o $train_tfrec_path\n",
        "!python src/generate_tfrecord_sk.py -x $valid_img_dir -l $label_map_path -o $valid_tfrec_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-09-02 23:08:28.483527: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Successfully created the TFRecord file: object_detection/tfrec_data/train.record\n",
            "2021-09-02 23:09:11.754892: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Successfully created the TFRecord file: object_detection/tfrec_data/valid.record\n"
          ]
        }
      ]
    }
  ]
}